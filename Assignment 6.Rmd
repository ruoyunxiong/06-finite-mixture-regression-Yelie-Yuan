---
title: "Assignment 6"
author: "Yelie Yuan"
date: "9/28/2019"
output:
  pdf_document:
    latex_engine: xelatex
  html_document: default
---

#### 1. Follow the lecture notes to verify the validity of the provided E- and M-steps. That is, derive the updating rules in the given algorithm based on the construction of an EM algorithm.


Density of $y_i$ is

$$
f(y_i\mid \mathbf{x}_i,\boldsymbol{\Psi})= \sum_{j=1}^{m} \pi_{j}\phi(y_i;\mathbf{x}_i^{\top}\boldsymbol{\beta}_{j}, \sigma^2),\qquad i=1,\ldots,n,
$$

So density of $y_i - \mathbf{x}_i^{\top}\boldsymbol{\beta}_{j}$ is 
$$
f(y_i - \mathbf{x}_i^{\top}\boldsymbol{\beta}_{j}\mid \boldsymbol{\Psi})= \sum_{j=1}^{m} \pi_{j}\phi(y_i - \mathbf{x}_i^{\top}\boldsymbol{\beta}_{j};0, \sigma^2),\qquad i=1,\ldots,n,
$$

From the nots, we have
$$
\begin{aligned}
Q(\theta | \theta_t) &= \sum_z p(\boldsymbol{z|x, \theta_t}) \ln p(\boldsymbol{x, z | \theta}) \\
& = \sum_{i = 1}^n \sum_{k = 1}^K p(z_i = k| \boldsymbol{x_i, \theta_t}) \ln p(z_i = k, \boldsymbol{x_i | \theta})
\end{aligned}
$$
In this exercise,

$$
Q(\boldsymbol{\Psi} | \boldsymbol{\Psi}^{(k)}) = \sum_{i = 1}^n \sum_{j = 1}^m p(z_{ij} | y_i, \boldsymbol{\mathbf{x}_i, {\Psi}^{(k)}}) \log p(z_{ij}, y_i,  \boldsymbol{\mathbf{x}_i | \Psi})
$$

By Bayes rule,


$$
\begin{aligned}
p(z_{ij} | y_i, \boldsymbol{\mathbf{x}_i, {\Psi}^{(k)}}) 
& = {p(z_{ij} , y_i, \boldsymbol{\mathbf{x}_i, |{\Psi}^{(k)}}) \over p(y_i, \boldsymbol{\mathbf{x}_i, |{\Psi}^{(k)}})} \\
& = {p(z_{ij} , y_i, \boldsymbol{\mathbf{x}_i, |{\Psi}^{(k)}}) \over \sum_{j = 1}^mp(z_{ij}, y_i, \boldsymbol{\mathbf{x}_i, |{\Psi}^{(k)}})} \\
& = {{\pi_{j}^{(k)}\phi(y_{i}-\boldsymbol{\mathbf{x}_{i}^{\top}\beta_{j}^{(k)}};0,\sigma^{2^{(k)}})} \over {\sum_{j=1}^{m}\pi_{j}^{(k)}\phi(y_{i}-\boldsymbol{\mathbf{x}_{i}^{\top}\beta_{j}^{(k)}};0,\sigma^{2^{(k)}})}} \\
& = p_{ij}^{(k + 1)} \\
p(z_{ij}, y_i, \boldsymbol{\mathbf{x}_i | \Psi}) & = {\pi_{j}\phi(y_{i}-\boldsymbol{\mathbf{x}_{i}^{\top}\beta_{j}};0,\sigma^2)}
\end{aligned}
$$

So we have 

$$
\begin{aligned}
Q(\boldsymbol{\Psi\mid \Psi^{(k)}})
& = \sum_{i = 1}^n \sum_{j = 1}^m p_{ij}^{(k + 1)} \lbrace \log  {\pi_{j} + \log \phi(y_{i}-\boldsymbol{\mathbf{x}_{i}^{\top}\beta_{j}};0,\sigma^2)} \rbrace \\
& = \sum_{i = 1}^n \sum_{j = 1}^m p_{ij}^{(k + 1)} \log \pi_j + 
\sum_{i = 1}^n \sum_{j = 1}^m p_{ij}^{(k + 1)} {1 \over \sqrt{2\pi \sigma^2}}
\exp \lbrace {-(y_i - \boldsymbol{\mathbf{x}_{i}^{\top}\beta_{j}})^2 \over 2\sigma^2} \rbrace \\
& = \sum_{i = 1}^n \sum_{j = 1}^m p_{ij}^{(k + 1)} \log \pi_j - {1 \over 2}
\sum_{i = 1}^n \sum_{j = 1}^m p_{ij}^{(k + 1)} \log 2\pi \sigma^2 - {1 \over 2}\sum_{i = 1}^n \sum_{j = 1}^m p_{ij}^{(k + 1)}
{(y_i - \boldsymbol{\mathbf{x}_{i}^{\top}\beta_{j}})^2 \over \sigma^2} \\
& = I_1 - {1 \over 2}I_2 - {1 \over 2}I_3 \\
\end{aligned}
$$

The M-Step maximizes $Q(\boldsymbol{\Psi\mid \Psi^{(k)}})$ , for $I_1$, we have

$$
\begin{cases}
\pi_1 + \pi_2 + \dots + \pi_m = 1 \\
L(\pi_1 \ldots, \pi_m) = \sum_{i = 1}^n \sum_{j = 1}^m p_{ij}^{(k + 1)} \log \pi_j
- \lambda \lbrace \sum_{j = 1}^m \pi_j - 1 \rbrace
\end{cases}
$$
with $\lambda$ a Lagrange multiplier. Then

$$
\pi_j =  {\sum_{i = 1}^n p_{ij}^{(k + 1)} \over 
\sum_{i = 1}^n \sum_{j = 1}^m p_{ij}^{(k + 1)} } = {1 \over n} \sum_{i = 1}^n p_{ij}^{(k + 1)}
$$

Only $I_3$ contains $\beta_j$, to minimize it, each $I_{3j}$ must be minimized. For each $\beta_j$, take derivitive of $I_{3j}$ and let it equal to 0. We have 
$$
\sum_{i = 1}^n p_{ij}^{(k + 1)}(-2 \mathbf{x}_i y_i + 2\mathbf{x}_i \mathbf{x}_i^\top \beta_j) = -2\sum_{i = 1}^n p_{ij}^{(k + 1)}\mathbf{x}_i y_i + 2\sum_{i = 1}^n p_{ij}^{(k + 1)}\mathbf{x}_i \mathbf{x}_i^\top \beta_j = 0
$$
Solve this equation, we have

$$
\beta_j = (\sum_{i = 1}^n \mathbf{x}_i \mathbf{x}_i^\top p_{ij}^{(k + 1)})^{-1} (\sum_{i = 1}^n \mathbf{x}_i p_{ij}^{(k + 1)} y_i)
$$

$I_2$ and $I_3$ contain $\sigma^2$, take the derivative of $I_2 + I_3$ with respect to $\sigma^2$ and let it equals 0, we have
$$
{1 \over \sigma^2} \sum_{i = 1}^n \sum_{j = 1}^m p_{ij}^{(k + 1)} - {1 \over \sigma^4} \sum_{i = 1}^n \sum_{j = 1}^m p_{ij}^{(k + 1)}
(y_i - \boldsymbol{\mathbf{x}_{i}^{\top}\beta_{j}})^2 = 0
$$
Then we have

$$
\sigma^2 = {\sum_{i = 1}^n \sum_{j = 1}^m p_{ij}^{(k + 1)}
(y_i - \boldsymbol{\mathbf{x}_{i}^{\top}\beta_{j}})^2 \over \sum_{i = 1}^n \sum_{j = 1}^m p_{ij}^{(k + 1)}} = {\sum_{i = 1}^n \sum_{j = 1}^m p_{ij}^{(k + 1)}
(y_i - \boldsymbol{\mathbf{x}_{i}^{\top}\beta_{j}})^2 \over n}
$$

To sum up, at the $k$th iteration, the updating rules are as follows (notice that we need to update $\beta_j^{(k + 1)}$ before we use it to update $\sigma^{2^{(k + 1)}}$)
$$
\begin{cases}
\pi_j^{(k + 1)} = {1 \over n} \sum_{i = 1}^n p_{ij}^{(k + 1)} \\
\beta_j^{(k + 1)} = (\sum_{i = 1}^n \mathbf{x}_i \mathbf{x}_i^\top p_{ij}^{(k + 1)})^{-1} (\sum_{i = 1}^n \mathbf{x}_i p_{ij}^{(k + 1)} y_i) \\
\sigma^{2^{(k + 1)}} = {\sum_{i = 1}^n \sum_{j = 1}^m p_{ij}^{(k + 1)}
(y_i - \boldsymbol{\mathbf{x}_{i}^{\top}\beta_{j}^{(k + 1)}})^2 \over n}
\end{cases}
$$

#### 2. Implement this algorithm in R with a function `regmix_em`. The inputs of the functions are `y`for the response vector, `xmat` for the design matrix, `pi.init` for initial values of $\pi_j$'s (a vector of $K\times 1$ vector), `beta.init` for initial values of $\boldsymbol{\beta}_j$'s (a matrix of $p \times K$ where $p$ is `ncol(xmat)` and $K$ is the number of components in the mixture), `sigma.init` for initial values of $\sigma$, and a `control` list for controlling max iteration number and convergence tolerance. The output of this function is the EM estimate of all the parameters.

In this part, we need to translate the prior maximization rules to codes. It will be more readable if we separate the rules as several parts. Firstly, write the following functions to update the parameters. `p_ij_fun` will be used to update $p_{ij}$, `beta_fun` will be used to update $\beta$, `sigma_fun` will be used to update $\sigma$. As for $\pi_j$, it's relatively easy to compute once we have $p_{ij}$, we will calculate it in the function `regmix_em`. And we need to get the length of $\pi_j$, number of components we have, for future use.

```{r}
p_ij_fun <- function(y, xmat, pi, beta, sigma) {
  m <<- length(pi)
  p_ij <- matrix(0, n, m)
  for (j in 1:m) {
    p_ij[, j] <- pi[j] * sapply(y - xmat %*% beta[, j], dnorm, mean = 0, sd = sigma)
  }
  return(p_ij / apply(p_ij, 1, sum))
}

beta_fun <- function(y, xmat, p_ij, beta) {
  for (j in 1:m) {
    temp1 <- 0
    temp2 <- 0
    p <- NROW(beta)
    for (i in 1:n) {
      temp1 <- temp1 + matrix(xmat[i, ], p, 1) %*% xmat[i, ] * p_ij[i, j]
      temp2 <- temp2 + matrix(xmat[i, ], p, 1) * p_ij[i, j] * y[i]
    }
    beta[, j] <- solve(temp1, temp2)
  }
  return(beta)
}

sigma_fun <- function(y, xmat, p_ij, beta) {
  temp3 <- 0
  for (j in 1:m) {
    temp3 <- temp3 + sum(p_ij[, j] * (y - xmat %*% beta[, j])^2)
  }
  return((temp3 / n)^0.5)
}
```

Then, `regmix_em` can be written as following. Set initial values of each patameter, store the results of $\beta_t$ and $\pi_t$ in $k$th iteration and use them to update the parameters in the next iteration. Once the code reached the max step or $\beta$ and $\pi$ stopped updating, the code will stops and return the parameters. There are four components in the result. The first one is our iteration steps, the second part is $\beta$, the third part is $\pi$, the last one is $\sigma$. 
```{r}
regmix_em <- function(y, xmat, pi.init, beta.init,
                      sigma.init, control) {
  tolerance <- 1
  steps <- 0
  xmat <- as.matrix(xmat)
  y <- as.matrix(y)
  pi <- pi.init
  beta <- beta.init
  sigma <- sigma.init
  while (steps < control$maxit & tolerance > control$tol) {
    beta_t <- beta
    pi_t <- pi
    
    p_ij <- p_ij_fun(y, xmat, pi_t, beta_t, sigma)
    pi <- apply(p_ij, 2, sum) / n
    beta <- beta_fun(y, xmat, p_ij, beta_t)
    sigma <- sigma_fun(y, xmat, p_ij, beta)
    
    steps <- steps + 1
    tolerance <- sum(abs(beta_t - beta)) + sum(abs(pi_t - pi))
  }
  return(list(Step = steps, Beta = beta, Pi = pi, Sigma = sigma))
}
```

#### 3. Here is a function to generate data from the mixture regression model.
```{r}
regmix_sim <- function(n, pi, beta, sigma) {
    K <- ncol(beta)
    p <- NROW(beta)
    xmat <- matrix(rnorm(n * p), n, p) # normal covaraites
    error <- matrix(rnorm(n * K, sd = sigma), n, K)
    ymat <- xmat %*% beta + error # n by K matrix
    ind <- t(rmultinom(n, size = 1, prob = pi))
    y <- rowSums(ymat * ind)
    data.frame(y, xmat)
}
```
Generate data with the following and estimate the parameters.
```{r}
n <- 400
pi <- c(.3, .4, .3)
bet <- matrix(c( 1,  1,  1, 
                -1, -1, -1), 2, 3)
sig <- 1
set.seed(1205)
dat <- regmix_sim(n, pi, bet, sig)
## regmix_em(y = dat[,1], xmat = dat[,-1], 
##           pi.init = pi / pi / length(pi),
##           beta.init = bet * 0,
##           sigma.init = sig / sig, 
##           control = list(maxit = 500, tol = 1e-5))
```


Use the following code and initial values to estimate the parameters. 
```{r}
regmix_em(y = dat[,1], xmat = dat[,-1], 
          pi.init = pi / pi / length(pi),
          beta.init = matrix(rnorm(6), 2, 3),
          sigma.init = sig / sig, 
          control = list(maxit = 500, tol = 1e-5))
```

We can set the initial values of $\beta$ randomly, but the order of $\beta_j$ in our results might be different. The original $\beta$ we used to generate $y$ is $\beta_1 = [1; 1], \beta_2 = [1; -1], \beta_3 = [-1; -1]$. But our estimated $\beta_j$ might have a different order according to the original oder, for example our estimated result can be $\beta_1 \approx [-1; -1], \beta_2 \approx [1; -1], \beta_3 \approx [1; 1]$. But it doesn't mean our estimated results are worng. If we check $p_{ij}$, the possibilities of every observation from each component, we can find that the values are basically same. The different oder is just different sequence of group. In general, the same group still has the same "members".